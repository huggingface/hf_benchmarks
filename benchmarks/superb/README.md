# SUPERB

## Submission workflow

SUPERB accepts two types of submission:

* **Pretrained model upload:** Participants submit pretrained model weights (e.g. HuBERT) and the SUPERB team fine-tunes this model on all the downstream tasks. Each fine-tuned model is then evaluated on the hidden test set associated with each task. Note that the pretrained model must be compatible with the `s3prl` framework 
* **Fine-tuned model upload:** Participants conduct their own fine-tuning experiments on the public train and dev sets, and submit the fine-tuned model weights and hyperparameters.

Below we describe the submission workflow for each type.

### Pretrained model upload

Here participants should take the following steps:

1. Fork the [`pretrained-model-upload-template`](https://huggingface.co/superb/pretrained-model-upload-template) model repo on the Hugging Face Hub
2. Populate it with pretrained model weights and the `UpstreamExpert` interface defined in `expert.pt`
3. Push the files to the Hub

Once a pretrained model is pushed to the Hub the following steps are taken:

1. The model if fine-tuned on various downstream tasks using the [`s3rl` framework](https://github.com/s3prl/s3prl).
2. Each fine-tuned model is submitted to the Hugging Face Inference API to generate predictions on the hidden test set.
3. The predictions for each task are then used with the hidden test labels to compute metrics and populate the leaderboard.

#### TODO

- [ ] Implement validation script that checks for filename and extensions needed for fine-tuning
- [ ] Implement script that automates the submission process by creating a new branch, adding LFS tracking etc.
- [ ] Implement logic in inference script to collect all downstream models associated with a submission ID and submit inference jobs
- [ ] Implement logic in evaluation to collect all prediction repos associated with a submission ID and collect evaluation metrics in one place

### Fine-tuned model upload

Here the participants should take the following steps:

1. Fork the [`finetuned-model-upload-template`](https://huggingface.co/superb/finetuned-model-upload-template) model repo on the Hugging Face Hub
2. Fine-tune their single pretrained model on all downstream tasks.
3. Store the fine-tuned model weights and hyperparameters in the corresponding repo directory.
4. Commit and push the files to the Hub.

One all fine-tuned models are pushed to the Hub:

1. Each fine-tuned model is submitted to the Hugging Face Inference API to generate predictions on the hidden test set.
2. The predictions for each task are then used with the hidden test labels to compute metrics and populate the leaderboard.


## Developer installation

Clone the repository and install the requirements:

```
git clone git@github.com:huggingface/evaluate.git
cd evaluate
pip install '.[superb]'
```

## Usage

### Generate predictions

To generate the predictions from a model that is fine-tuned on one of SUPERB's downstream tasks, run

```bash
HF_HUB_TOKEN=yourToken python inference.py 'model_id' 'superb' 'downstream_task' 'dataset_split' 'dataset_column'
```

where `HF_HUB_TOKEN` is the API token associated with your Hugging Face Hub account, `model_id` is the name of the fine-tuned model, and `dataset_column` is the name of the column from which to run inference over.

This sends a job to Hugging Face's inference API. If successful, you should received a message like:

```
Launching inference job with ID bulk-e13a0f17 for model lewtun/superb-s3prl-osanseviero__hubert_base-diarization-7f28b8b5 on dataset superb! Dataset repo created with name lewtun/bulk-superb-s3p-superb-8e373
```

and the resulting predictions will be stored under your user account.

#### TODO

- [ ] Standardise the prediction filename outputs so they can be easily loaded during evaluation.
- [ ] Allow predictions to be generated by organizations (e.g. SUPERB)

## Datasets and metrics

You can load each task of the SUPERB benchmark using the `datasets` library as follows:

```python
from datasets import load_dataset

dset = load_dataset("superb", "downstream_task")
```

Currently the supported set of tasks are `asr` and `sd` (speaker diarization). Similarly you can load the metrics associated with each downstream task:

```python
from datasets import load_metric

metric = load_metric("metric_name")
```

